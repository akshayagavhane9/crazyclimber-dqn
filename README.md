# ğŸ§  Deep Q-Learning Agent â€” Atari *Crazy Climber*

![Python](https://img.shields.io/badge/Python-3.11-blue)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red)
![Gymnasium](https://img.shields.io/badge/Env-Gymnasium-orange)
![License: MIT](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Complete-success)

---

## ğŸ® Project Overview

This project implements a **Deep Q-Learning (DQN)** agent to master the Atari game **CrazyClimber-v5**, built with **PyTorch**, **Gymnasium**, and **Atari Preprocessing** wrappers.

The agent learns to climb using reinforcement learning â€” observing frames, predicting Q-values for each action, and maximizing long-term rewards.  
It is part of the **LLM Agents & Deep Q-Learning** academic assignment, focusing on agent optimization, hyperparameter analysis, and policy exploration.

---

## ğŸš€ Key Features

âœ… **DQN Implementation**
- Experience Replay + Target Network updates  
- Îµ-Greedy and Softmax exploration  
- Huber loss and Adam optimizer  
- FrameStack (4 Ã— 84Ã—84 grayscale frames)

âœ… **Experimentation Framework**
- Configurable Î³, Î±, Îµ decay rates, learning rate  
- Variant runs for hyperparameter sweeps  
- Visualization of metrics (returns, loss, steps)

âœ… **Portfolio-Ready Deliverables**
- Jupyter notebook with analysis and plots  
- Scripts for environment inspection & reward probing  
- Documentation, licensing, and attribution files  

---

## ğŸ§© Results Summary

| Variant | Avg Return (last 10 eps) | Avg Steps | Highlights |
|----------|--------------------------|------------|-------------|
| **Baseline (Î³ = 0.99)** | **15,010** | 2,557 | Stable convergence |
| **Î³ = 0.95** | 18,340 | 2,861 | Faster initial learning |
| **Î³ = 0.999 + lr 5e-5** | 8,220 | 2,348 | Slower, underfitted |
| **Softmax (Ï„ = 1.0)** | 24â€“30 k | 3,000 | Strong exploration & reward spikes |
| **Fast Îµ Decay (20 ep)** | 11,380 | 2,834 | Rapid exploration, less stability |
| **Slow Îµ Decay (80 ep)** | 12,010 | 2,367 | Gradual learning, consistent returns |

---

## ğŸ“Š Visual Insights

Key metrics visualized in the notebook:
- ğŸ“ˆ **Episode Return vs. Steps**
- ğŸ“‰ **Loss Convergence Curve**
- ğŸ¯ **Epsilon Decay Schedule**
- ğŸªœ **FrameStack Visualization (84Ã—84 grayscale)**  
- ğŸ§© **Reward Distribution Probe**
- ğŸ§  **LLM-Agent Integration Diagram**

---

## ğŸ§  Concepts Demonstrated
- Markov Decision Processes (MDPs)  
- Bellman Optimality & Q-Value updates  
- Experience Replay & Stability in Training  
- Explorationâ€“Exploitation trade-off  
- Hyperparameter tuning & analysis  

---

## âš™ï¸ Quick Start

```bash
# 1ï¸âƒ£ Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate    # macOS/Linux
# or .venv\Scripts\activate   # Windows

# 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Verify setup
python -m src.smoke_test

# 4ï¸âƒ£ Train the baseline agent
python train_dqn.py

# 5ï¸âƒ£ Evaluate performance
python eval_baseline.py

# 6ï¸âƒ£ Run variants (optional)
python scripts/run_variant.py --out_dir runs/gamma_0_95 --gamma 0.95
```

## ğŸ“‚ Project Structure

crazyclimber-dqn/
â”œâ”€â”€ src/           # Core implementation
â”œâ”€â”€ scripts/       # Experiment and analysis scripts
â”œâ”€â”€ runs/          # Saved checkpoints & logs
â”œâ”€â”€ notebook.ipynb # Main experiment notebook
â”œâ”€â”€ LICENSE
â”œâ”€â”€ ATTRIBUTION.md
â””â”€â”€ README.md


---

## ğŸªª License & Attribution
MIT License Â© 2025 **Akshaya Gavhane**  
Developed for the *LLM Agents & Deep Q-Learning* course at Northeastern University.  
See [LICENSE](./LICENSE) and [ATTRIBUTION.md](./ATTRIBUTION.md) for details.
